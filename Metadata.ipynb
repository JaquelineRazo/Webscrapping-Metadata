{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Obtener metadata de página web\n",
        "\n",
        "# Librerias\n",
        "\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "#specifica el path al chromedriver.exe (tienes que descargar la ultima versión estable en tu compu)\n",
        "driver = webdriver.Chrome('....../chromedriver')\n",
        "# especifica la página que necesites, te recomiendo que hagas una lista de todos los links de categorías de Local.mx y posteriormente crees una función que itere cada uno de los elementos de esa lista.\n",
        "\n",
        "driver.get('https://xxxxxxxx')\n",
        "\n",
        "# Carga más notas\n",
        "driver.execute_script(\"window.scrollTo(0,650000);\")\n",
        "\n",
        "#Obten los datos que necesites, yo solo sacaba título, autor, fecha y urls\n",
        "\n",
        "titulos = []\n",
        "titulos_ = driver.find_elements_by_class_name('grid-item-title')\n",
        "for t in titulos_:\n",
        "    title = t.find_element_by_xpath('./a').text     \n",
        "    titulos.append(title)\n",
        "\n",
        "\n",
        "fechas = []\n",
        "fecha_ = driver.find_elements_by_xpath('//span[@class=\"grid-item-date small-uppercase\"]')\n",
        "for t in fecha_:\n",
        "    f = t.find_element_by_xpath('.//small').text     \n",
        "    fechas.append(f)\n",
        "\n",
        "urls = []\n",
        "url_ = driver.find_elements_by_class_name('grid-item-title')\n",
        "for t in url_:\n",
        "    f = t.find_element_by_xpath('./a').get_attribute('href')\n",
        "    urls.append(f)\n",
        "\n",
        "author = []\n",
        "\n",
        "for i in urls:\n",
        "    r = requests.get(i)\n",
        "    home=r.content.decode('utf-8')\n",
        "    parser=html.fromstring(home)\n",
        "    autor_ = parser.xpath(autor)\n",
        "    author.append(autor_)\n",
        "\n",
        "\n",
        "# Crea un DF con toda la info\n",
        "titulos_df = pd.DataFrame(titulos, columns =['Título'])\n",
        "links_df = pd.DataFrame(urls, columns =['URL'])\n",
        "fecha_publicacion_df = pd.DataFrame(fechas, columns =['Fecha de Publicación'])\n",
        "autor_df = pd.DataFrame(author, columns =['Autor'])\n",
        "frames = [titulos_df,links_df,fecha_publicacion_df,autor_df]\n",
        "donde_comprar = pd.concat(frames, axis=1)\n",
        "donde_comprar\n",
        "\n",
        "\n",
        "# Descargalo o envíalo a google sheets\n",
        "donde_comprar.to_excel('/Users/user/Desktop/XXXXXXXXX.xlsx', index=False)"
      ],
      "metadata": {
        "id": "RaA6ByER-EqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}